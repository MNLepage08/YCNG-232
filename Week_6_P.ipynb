{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Build a auto-corrector based on a (small) vocabulary of ”known words” extracted from a text repository of your choice that takes as input a sentence (with possible misspelled words) and replaces each of the words with one from the vocabulary that minimizes the edit distance.\n",
        "Please cite your sources, show your code, and include some input-output examples.\n",
        "Discuss what other techniques (that we have discussed in previous sessions or that you can think of otherwise) could be used to make this simple auto-correct perform better in terms of inferring the intended meaning of the word or to take into account similarities in pronunciation between differently-spelled words.**"
      ],
      "metadata": {
        "id": "jcZQX5LD0Kdm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/code/bouweceunen/levenshtein-distance-spelling-correction-nlp/notebook\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2021/06/how-to-load-kaggle-datasets-directly-into-google-colab/"
      ],
      "metadata": {
        "id": "6FKwE6NW0QGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Kaggle library\n",
        "! pip install kaggle"
      ],
      "metadata": {
        "id": "8lbypF7a6dRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a directory named “.kaggle”\n",
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "VIGfayY96hvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the “kaggle.json” into this new directory\n",
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "Iabz0-s36m5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Allocate the required permission for this file.\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "v2fj6FAE6sBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading Datasets: https://www.kaggle.com/datasets/bouweceunen/smart-home-commands-dataset\n",
        "! kaggle datasets download bouweceunen/smart-home-commands-dataset"
      ],
      "metadata": {
        "id": "gnnYX_NN9Aq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the file in Google Colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "wQLJw6BW_IDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('smart-home-commands-dataset.zip')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "wHyCx_XN_QSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62FPhxuA0JHM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from nltk import word_tokenize\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_sent = df[['Sentence']]\n",
        "df_sent.head(10)"
      ],
      "metadata": {
        "id": "KVfAlcYj0XYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize each sentence\n",
        "sent = [word_tokenize(sentence['Sentence']) for index, sentence in df_sent.iterrows()]\n",
        "sent[0]"
      ],
      "metadata": {
        "id": "Gz-cLu1mCQG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge each word of the sentences togheter \n",
        "merge_sent = list(itertools.chain.from_iterable(sent))\n",
        "print(merge_sent)"
      ],
      "metadata": {
        "id": "qT-5gZ-aDMNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distinct word voccabulary to know\n",
        "vocabulary = list(set(merge_sent))\n",
        "print(vocabulary)"
      ],
      "metadata": {
        "id": "bkMBADswDfix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Levenshtein distance\n",
        "\n",
        "def editdist(p, q, elimination = 1, insertion = 1, defrep = 1, repcost = dict()):\n",
        "    d = dict()\n",
        "    np = len(p) + 1 # length of first string plus one \n",
        "    nq = len(q) + 1 # length of second string plus one\n",
        "    for i in range(np): # initialize each row\n",
        "        d[(i, 0)] = i * insertion\n",
        "    for j in range(nq): # initialize each column\n",
        "        d[(0, j)] = j * elimination\n",
        "    for i in range(1, np):\n",
        "        for j in range(1, nq):\n",
        "            lp = p[i - 1] # corresponding letter of the first string\n",
        "            lq = q[j - 1] # corresponding letter of the second string\n",
        "            eli = d[(i - 1, j)] + elimination\n",
        "            ins = d[(i, j - 1)] + insertion\n",
        "            ree = d[(i - 1, j - 1)] # no cost of replacement unless they differ\n",
        "            if lp != lq:\n",
        "              # include cost of that pair or default cost if undefined\n",
        "              ree += repcost.get((lp, lq), defrep) \n",
        "            d[(i, j)] = min(eli, ins, ree) # dynamic programming step: the cheapest option wins\n",
        "    return d[(np -1, nq - 1)] # final cost\n",
        " \n",
        "print(editdist(\"orthography\", \"ortografy\"))"
      ],
      "metadata": {
        "id": "mOS3f49kEjF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def auto_correction(sentence):\n",
        "\n",
        "  # Tokenize the sentence to auto_correction by word\n",
        "  wt = word_tokenize(sentence)\n",
        "\n",
        "  # For each word in the sentence\n",
        "  for i, word in enumerate(wt):\n",
        "\n",
        "        # If the word is not in the know word of the text and not digit\n",
        "        if (word not in vocabulary and not word.isdigit()): # ignore digits\n",
        "\n",
        "            # Create a list\n",
        "            levdistances = []\n",
        "\n",
        "            # Calcul the Levenshtein distance for each word to know\n",
        "            for j in vocabulary:\n",
        "\n",
        "              # Put the distance in the list\n",
        "              levdistances.append(editdist(word,j))\n",
        "\n",
        "              # Take the word with the minimum distance\n",
        "              wt[i] = vocabulary[levdistances.index(min(levdistances))]\n",
        "\n",
        "        else:\n",
        "          # If the word is a know word in the voccabulary (no auto-correction)\n",
        "          wt[i] = word\n",
        "\n",
        "  return ' '.join(wt)"
      ],
      "metadata": {
        "id": "hl25-zXNAewA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Illumminate & kitchean & todday corrected\n",
        "print(auto_correction(\"Illumminate the kitchean todday.\"))"
      ],
      "metadata": {
        "id": "bdfhZCm5AqGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Turne & lihght & inn corrected\n",
        "print(auto_correction(\"Turne on the lihght in the kitchen inn 1 day.\"))"
      ],
      "metadata": {
        "id": "5R43NR5lGJDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using either some n-gram based or another type of approach (remember to cite any sources you consult) and a text repository of your choice, implement a simple auto-complete that suggests possible options for what the next word could be, given a start of a sentence as input.\n",
        "Please include code snippets and examples, as usual.\n",
        "Discuss how the value for n affects the quality you observe (subjective or measured). Would\n",
        "you actually need a range of values for n instead of a single value for this to work well?**"
      ],
      "metadata": {
        "id": "noNMLcICNNNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.nltk.org/howto/collocations.html"
      ],
      "metadata": {
        "id": "WMNnIFNGNk6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(merge_sent)"
      ],
      "metadata": {
        "id": "aqHqlWSZ-Ljd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from heapq import nlargest\n",
        "from operator import itemgetter"
      ],
      "metadata": {
        "id": "geXL8FZ7HNW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def auto_complete_tri(sentence, n):\n",
        "\n",
        "  find = nltk.TrigramCollocationFinder.from_words(merge_sent) \n",
        "  pmi = find.score_ngrams(nltk.TrigramAssocMeasures().pmi)\n",
        "\n",
        "  beg_sentence = nltk.word_tokenize(sentence)\n",
        "  w1 = beg_sentence[0]\n",
        "  w2 = beg_sentence[1]\n",
        "\n",
        "  top=[]\n",
        "  for (tri, score) in pmi:\n",
        "    (first, second, third) = tri\n",
        "\n",
        "    if first == w1 and second == w2:\n",
        "      top.append((third, score))\n",
        "    \n",
        "  top_max = nlargest(n, top, key=itemgetter(1))\n",
        "\n",
        "  return (top_max)"
      ],
      "metadata": {
        "id": "keog2vleAIia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auto_complete_tri('Illuminate the', 5)"
      ],
      "metadata": {
        "id": "IlGOhnQsDjxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auto_complete_tri('Give me', 5)"
      ],
      "metadata": {
        "id": "MapcRUVNEYq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auto_complete_tri('Open the', 5)"
      ],
      "metadata": {
        "id": "ZT167yvdOAWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def auto_complete_bi(word, n):\n",
        "\n",
        "  find = nltk.BigramCollocationFinder.from_words(merge_sent) \n",
        "  pmi = find.score_ngrams(nltk.BigramAssocMeasures().pmi)\n",
        "\n",
        "  top=[]\n",
        "  for (second, score) in pmi:\n",
        "    (first, second) = second\n",
        "\n",
        "    if first == word:\n",
        "      top.append((second, score))\n",
        "    \n",
        "  top_max = nlargest(n, top, key=itemgetter(1))\n",
        "\n",
        "  return (top_max)"
      ],
      "metadata": {
        "id": "QMRz-Gi3Kiv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auto_complete_bi('the', 15)"
      ],
      "metadata": {
        "id": "0zftzHC2LBnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modify your auto-complete so that it never suggests names of people or places. Include a code snippet and examples in your response.**"
      ],
      "metadata": {
        "id": "gbEatU5gMoXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.analyticsvidhya.com/blog/2021/06/nlp-application-named-entity-recognition-ner-in-python-with-spacy/,"
      ],
      "metadata": {
        "id": "K7sMfzbZiykZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "NER = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "wCAG4gToiyJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_nt = [sentence['Sentence'] for index, sentence in df_sent.iterrows()]\n",
        "print(sent_nt)"
      ],
      "metadata": {
        "id": "C4fLJrHLS3l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ' '.join(sent_nt)\n",
        "print(text)"
      ],
      "metadata": {
        "id": "WiCXsBEYjPR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1= NER(text)"
      ],
      "metadata": {
        "id": "JK2KF3YdjA21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name=[]\n",
        "for word in text1.ents:\n",
        "    print(word.text,word.label_)\n",
        "    if word.label_ == 'ORG' or word.label_ == 'PERSON' or word.label_ == 'LOC' or word.label_ == 'GPE':\n",
        "      name.append(word.text)"
      ],
      "metadata": {
        "id": "5lIrhTmQjqt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names = list(set(name))\n",
        "print(names)"
      ],
      "metadata": {
        "id": "cGLxm3bdkYrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names.append('Facebook')\n",
        "print(names)"
      ],
      "metadata": {
        "id": "ZKyYzEvMn2fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "FKUQhJNZpqVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = ' '.join(names)\n",
        "names = nltk.word_tokenize(n)\n",
        "names = list(set(names))\n",
        "print(names)"
      ],
      "metadata": {
        "id": "C96xs-MGpUVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def auto_complete_tri(sentence, n):\n",
        "\n",
        "  find = nltk.TrigramCollocationFinder.from_words(merge_sent) \n",
        "  pmi = find.score_ngrams(nltk.TrigramAssocMeasures().pmi)\n",
        "\n",
        "  beg_sentence = nltk.word_tokenize(sentence)\n",
        "  w1 = beg_sentence[0]\n",
        "  w2 = beg_sentence[1]\n",
        "\n",
        "  top=[]\n",
        "  for (tri, score) in pmi:\n",
        "    (first, second, third) = tri\n",
        "\n",
        "    if first == w1 and second == w2 and third not in names:\n",
        "      top.append((third, score))\n",
        "    \n",
        "  top_max = nlargest(n, top, key=itemgetter(1))\n",
        "\n",
        "  return (list(zip(*top_max))[0])"
      ],
      "metadata": {
        "id": "klwa4DXvqDmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auto_complete_tri('Open the', 5)"
      ],
      "metadata": {
        "id": "UQEIfPabqGob"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}