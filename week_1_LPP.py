# -*- coding: utf-8 -*-
"""Week_1_LPP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16oEoRX-99b2bJReBV9KxY664WDsg5oKl

**1.	What data did you use? If it is a book from Gutenberg, which one?**

Load the text file Le Petit Prince from the Web
"""

from urllib.request import urlopen
url = "http://lepetitprinceexupery.free.fr/telecharger/le-petit-prince--antoine-de-saint-exupery.txt"
text = urlopen(url).read().decode("Windows-1252")
text[:100]

"""Take only the text"""

marker = 'lepetitprinceexupery.free.fr\n****************************\n\n\n'
startPosition = text.index(marker) + len(marker)
content = text[startPosition:]

marker = '\n\n\n\n\n****************************\nlepetitprinceexupery.free.fr'
endPosition = text.index(marker) - len(marker)
content1 = content[:endPosition]
content1[-600:]

content1

"""Segmentation by chapters"""

chapters = content1.split('hapitre ')
print(len(chapters), 'chapters ???')
# Problem with the chapter 4 (write chapitre and not Chapitre)

import re # a regular-expression library
four = re.sub('chapitre 4', 'Chapitre 4', content1)
chapters = four.split('Chapitre ')
chapters[4]

nodigits = re.sub(r'[0-9]+', '', four)
#print('No digits:', nodigits[:100])
clean = re.compile(r'\s+') # also combine any kind of repeated whitespace into a single space
ok = clean.sub(' ', nodigits)
#print('Cleaned:', ok[:100])
chapters_split = ok.split('Chapitre ')
chapters = [ s for s in chapters_split if len(s) > 0] # keep only the ones with content
len(chapters)

chapters[26]

"""**2.	Can you estimate how many sentences it contains? What can you do to compute this (conceptually, in Python and/or in R)?**

Segmentation by sentence
"""

import nltk
from nltk.tokenize import sent_tokenize
nltk.download('punkt')

sent, cnt = [], []
for chap in chapters:
  s=sent_tokenize(chap)
  c=len(s)
  sent.append(s)
  cnt.append(c)
sent[0]

# Sentence for each chapter
print(cnt)
len(cnt)

#Total sentence in the document
print('Number of sentences in document:' ,sum(cnt))

"""**3. How about paragraphs? Can you estimate how many there are? What can you do to compute this (conceptually, in Python and/or in R)?**

Segmentation by paragraph
"""

nodigits = re.sub(r'[0-9]+', '', four)
#print('No digits:', nodigits[:100])
#clean = re.compile(r'\s') # also combine any kind of repeated whitespace into a single space
#ok = clean.sub(' ', nodigits)
#print('Cleaned:', ok[:100])
chapters_split = nodigits.split('Chapitre ')
chapters = [ s for s in chapters_split if len(s) > 0] # keep only the ones with content
len(chapters)

chapters[0]

"""Ainsi, les paragraphe sont séparé par \n\n"""

paragraphes, cnt = [],[]
for para in chapters:  
  p = list(filter(lambda x : x != '', para.split('\n\n')))
  c = len(p)
  paragraphes.append(p)
  cnt.append(c)
paragraphes[0]

# Paragraph for each chapter
print(cnt)
len(cnt)

#Total paragraphs in the document
print('Number of paragraphs in document:' ,sum(cnt))

"""**4. How many names (of places and people) are mentioned in the text? What did you do to compute this (conceptually, in Python and/or in R)?**"""

import operator
for c in chapters:
  freq = { w : c.count(w) for w in c.split() } # build a dictionary
  top = max(freq.items(), key = operator.itemgetter(1))[0] # the most frequent word
  longest = max(freq.keys(), key = len) # the longest word
  print(top, longest)

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
skip = stopwords.words('french')
print(skip)

from string import punctuation
for chapter in chapters:
  nopunct = chapter.translate(str.maketrans(punctuation, ' ' * len(punctuation))) # substitute with space
  clean = re.compile(r'\s+')
  ok = clean.sub(' ', nopunct) # in case we made repeated spaces
  words = ok.split()
  freq = { w : words.count(w) for w in words if w.lower() not in skip } # build a dictionary of the non-stopwords
  top = max(freq.items(), key = operator.itemgetter(1))[0] # the most frequent word
  longest = max(freq.keys(), key = len) # the longest word
  print(top, longest)

from string import printable # this does not contain mdash & those angled quotes 
print('OK:', printable)
nonprint = f'[^{re.escape(printable)}]'

words = set() # let's collect all the words
for chapter in chapters:
  nopunct = chapter.translate(str.maketrans(punctuation, ' '*len(punctuation)))
  #better = re.sub(nonprint, ' ', nopunct)
  clean = re.compile(r'\s+')
  ok = clean.sub(' ', nopunct) # in case we made repeated spaces
  words.update(set(ok.split())) # include these new words in the set

names = set()
regular = set()
for w in words:
  if len(w) < 3:
    continue # too short for our taste
  u = w.upper() # all uppercase version
  l = w.lower() # all lowercase version
  if w == u: # the word WAS all uppercase
    w = l.capitalize() # just a capital initial, then
  if l in skip or u in skip or w in skip:
    continue # ignore stop words
  if l in words: # not a name since it also appears in lowercase
    regular.add(l) # only keep the lowercase version
  else:
    names.add(w)
print(names)

very_names = ('Nord', 'Mars', 'Russie', 'France', 'Chine', 'Dieu', 'États-Unis','Noël','Jupiter','Majesté','Sibérie','Afrique','Sud','Arizona','Amérique du Sud','Amérique du Nord','Europe','Indes','Pacifique','Nouvelle-Zélande','Vénus','Australie','Sahara','Sire')
very_names

len(very_names)

"""Expérience infructueuse avec translate et pos_tag..."""

!pip install googletrans==3.1.0a0

# import libraries
import googletrans
from googletrans import Translator
import pandas as pd

# show languages
print(googletrans.LANGUAGES)

# use method translate to translate english into japanese
# create a translator object
translator = Translator()
translated_ita = translator.translate('おはようございます。', src='ja', dest='en')
translated_ita.text

wrds_en = translator.translate(names, src='fr', dest='en')
we = wrds_en.text
we

nopunct = we.translate(str.maketrans(punctuation, ' ' * len(punctuation))) # substitute with space
clean = re.compile(r'\s+')
ok = clean.sub(' ', nopunct) # in case we made repeated spaces
ok

t = ok.split()
l = [ s for s in t if len(s) > 0]
print(l)

from nltk.tag import pos_tag, pos_tag_sents
nltk.download('averaged_perceptron_tagger')
tag = nltk.pos_tag(l)

print(tag)

l=['Look']
m=['Ask']
o=['Look', 'Ask']
p=['Look','me','please','!']

lp = nltk.pos_tag(l)
lm = nltk.pos_tag(m)
lo = nltk.pos_tag(o)
ln = nltk.pos_tag(p)

print(lp,lm,lo,ln)

"""https://www.nltk.org/book/ch05.html: 
Many words, like ski and race, can be used as nouns or verbs with no difference in pronunciation. Can you think of others? Hint: think of a commonplace object and try to put the word to before it to see if it can also be a verb, or think of an action and try to put the before it to see if it can also be a noun. Now make up a sentence with both uses of this word, and run the POS-tagger on this sentence.

**5. What are the ten most frequent words in the text that you consider to clearly be stop words?**

Par chapitre
"""

top = 10
ok1='' 
text1 = ''

for chapter in chapters:
  nopunct = chapter.translate(str.maketrans(punctuation, ' ' * len(punctuation)))
  #better = re.sub(nonprint, ' ', nopunct)
  ok = clean.sub(' ', nopunct) 
  text1 += ' ' + ok
  words = ok.split()
  freq = { w.lower() : words.count(w) for w in words if w.lower() in skip } # skip names, too, and make these lowercase
  highest = sorted(freq, key = freq.get, reverse = True)[:top]
  print(highest)

print(skip)

text2 = text1.split()
freq = { w : text2.count(w) for w in text2 if w.lower() in skip }
f = nltk.FreqDist(freq)
print(f.most_common(10))

"""**7. How would you go about automatically identifying potential stop words for documents
written in a language you do not speak?**
"""

top = 10
for chapter in chapters:
  nopunct = chapter.translate(str.maketrans(punctuation, ' ' * len(punctuation)))
  #better = re.sub(nonprint, ' ', nopunct)
  ok = clean.sub(' ', nopunct) 
  words = ok.split()
  freq = { w.lower() : words.count(w) for w in words if w.lower() not in skip and len(w)>2} # skip names, too, and make these lowercase
  highest = sorted(freq, key = freq.get, reverse = True)[:top]
  print(highest)

skip += ['tout','très','comme','bien','plus','rien','peu','encore','alors','quel','elles','quand','cette','donc','trop']
freq = { w : text2.count(w) for w in text2 if w.lower() not in skip and len(w)>2}
f = nltk.FreqDist(freq)
print(f.most_common(10))

"""**8. Looking at a histogram of the word frequencies (like the horizontal bar chart in the R example code), what can be said about the shape of the distribution?**"""

import collections
import operator
word_cnt = collections.Counter(f)
count_words, count_values = zip(*word_cnt.items())
values_sorted, words_sorted = zip(*sorted(zip(count_values, count_words), key=operator.itemgetter(0), reverse=True))

words_sorted_top = words_sorted[0:10]
values_sorted_top = values_sorted[0:10]

import pandas as pd
df = pd.DataFrame({'count': values_sorted_top, 'word': words_sorted_top})
df

import matplotlib.pyplot as plt
#% matplotlib inline

# creating the data values for the vertical y and horisontal x axis
y = df.iloc[:,0].sort_index(ascending=False)
x = df.iloc[:,1].sort_index(ascending=False)

fig = plt.figure()
fig.suptitle('Word frequency histogram, top {0}'.format(10), fontsize=16)
plt.xlabel('word', fontsize=12)
plt.ylabel('count', fontsize=12)

# using the pyplot.barh funtion for the horizontal bar
plt.barh(x,y)
plt.grid()

# to show our graph
plt.show()

"""**9. Please show an example of a word cloud you created from your text along with a code snippet of how you did it.**"""

from wordcloud import WordCloud, ImageColorGenerator

draw = ' '.join([ w.lower() for w in text2 if w.lower() not in skip and len(w) > 2 ])
cloud = WordCloud().generate(draw)
plt.imshow(cloud)
plt.axis('off')
plt.show()