{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Based on a word network, write a program that takes a sentence and exchanges each word to another one with a similar meaning, if one exists. You are free to exclude stop words and names from this transformation.\n",
        "Please include a code snippet and at least three example inputs with their respective outputs in your response.\n",
        "Discuss how legible (in the sense of easy to understand) you find the transformed texts in comparison to the originals.**"
      ],
      "metadata": {
        "id": "pmkXtZOkEiZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk # this we already have\n",
        "nltk.download('wordnet') # this is new, download once per environment\n",
        "nltk.download('omw-1.4') # same here\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3UfVH16UpMR",
        "outputId": "5deb8f5a-817f-404f-ca21-288688f7456f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_1 = 'The roof of my house needs repair.'\n",
        "sent_2 = 'The pretty birds eat vegetables from my garden.'\n",
        "sent_3 = 'This lakeside cottage is amazing.'"
      ],
      "metadata": {
        "id": "Tp3293bgXp_E"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#Stopword\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQzNY6RTeRgV",
        "outputId": "37009188-078c-4859-bdbb-bc9401b8516a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def change_word(sentence):\n",
        "\n",
        "  sent=[]\n",
        "\n",
        "  # Tokenize the sentence\n",
        "  sent_1w = word_tokenize(sentence)\n",
        "\n",
        "  for i in range(0, len(sent_1w)):\n",
        "\n",
        "    # check stopwords and change nothing if is in the list of stopwords\n",
        "    if sent_1w[i] in stop_words:\n",
        "      sent.append(sent_1w[i])\n",
        "  \n",
        "    else:\n",
        "      synset = wn.synsets(sent_1w[i].lower())\n",
        "\n",
        "      # check is the word have synset, if not append in the list\n",
        "      if len(synset) == 0:\n",
        "        sent.append(sent_1w[i])\n",
        "\n",
        "      # if the word have synset, take a similar word\n",
        "      else:\n",
        "        ss = synset[0] \n",
        "        lm = ss.lemma_names()\n",
        "        lm = lm[0]\n",
        "\n",
        "        # Choose lemma_names different of the word we want change\n",
        "        if lm != sent_1w[i]:\n",
        "          sent.append(lm)\n",
        "        else:\n",
        "          ss = synset[-1] \n",
        "          lm = ss.lemma_names()\n",
        "          lm = lm[-1]\n",
        "          sent.append(lm)\n",
        "\n",
        "  return sent"
      ],
      "metadata": {
        "id": "WFOXRkpAd6LI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentece 1\n",
        "print('Transform: ' + str(' '.join(change_word(sent_1))))\n",
        "print('Sentence: ' + str(sent_1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9r9WvkXOWne",
        "outputId": "65368d45-6744-4fde-897e-3869103f4cd4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transform: The roof of my domiciliate need revivify .\n",
            "Sentence: The roof of my house needs repair.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "synset = wn.synsets('roof')\n",
        "for batch in synset:\n",
        "  print(batch.lemma_names())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hfy4qBqeTK16",
        "outputId": "515d37b1-5c5b-44c4-9803-bbe8fca45ba2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['roof']\n",
            "['roof']\n",
            "['roof']\n",
            "['ceiling', 'roof', 'cap']\n",
            "['roof']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "synset = wn.synsets('needs')\n",
        "for batch in synset:\n",
        "  print(batch.lemma_names())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2b2gLDRW-qt",
        "outputId": "dff5e2a5-bb4b-4400-b017-e149b91d17ab"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['need', 'demand']\n",
            "['need', 'want']\n",
            "['motivation', 'motive', 'need']\n",
            "['indigence', 'need', 'penury', 'pauperism', 'pauperization']\n",
            "['necessitate', 'ask', 'postulate', 'need', 'require', 'take', 'involve', 'call_for', 'demand']\n",
            "['want', 'need', 'require']\n",
            "['need']\n",
            "['inevitably', 'necessarily', 'of_necessity', 'needs']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le mot roof n’a pas été changé. Dans ma boucle j’utilise le premier synset auquel le lemma_names est associé au même mot roof. Le mot needs a été remplacé par need. Globalement, la phrase transformée est similaire et fait du sens."
      ],
      "metadata": {
        "id": "Rs-HNicdVSU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentece 2\n",
        "print('Transform: ' + str(' '.join(change_word(sent_2))))\n",
        "print('Sentence: ' + str(sent_2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIB9xHfRQIcD",
        "outputId": "927eb93b-22c9-43ef-d55e-1d1a1f0bfb52"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transform: The passably bird rust vegetable from my garden .\n",
            "Sentence: The pretty birds eat vegetables from my garden.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "synset = wn.synsets('garden')\n",
        "for batch in synset:\n",
        "  print(batch.lemma_names())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYonL8rNYBu3",
        "outputId": "5965c1db-8980-4073-ec13-4f9ab63938fe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['garden']\n",
            "['garden']\n",
            "['garden']\n",
            "['garden']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le mot garden ne possède pas de lemma_nammes différent et reste ainsi. Globalement, la phrase ne fait plus vraiment de sens après la transformation."
      ],
      "metadata": {
        "id": "9W3GmkMFZkNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentece 3\n",
        "print('Transform: ' + str(' '.join(change_word(sent_3))))\n",
        "print('Sentence: ' + str(sent_3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Pw6O3iiQYK7",
        "outputId": "fdc1ee1f-0826-4ae3-ba3c-f00f19c761c2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transform: This lakeshore bungalow is amaze .\n",
            "Sentence: This lakeside cottage is amazing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La phrase transformé reste très similaire et compréhensible."
      ],
      "metadata": {
        "id": "XRGo-pGFZ1kF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "oExGkdfFRN1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# car.n.01 is called a synset, or “synonym set,” a collection of synonymous words (or “lemmas”)\n",
        "synset = wn.synsets('car')\n",
        "synset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b92FQq1WlGz",
        "outputId": "22ee6b14-a3b7-47bc-e3cc-ac5de018bd0c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('car.n.01'),\n",
              " Synset('car.n.02'),\n",
              " Synset('car.n.03'),\n",
              " Synset('car.n.04'),\n",
              " Synset('cable_car.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# collection of synonymous words (or “lemmas”):\n",
        "for batch in synset:\n",
        "  print(batch.lemma_names())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVGA8ugBXEDA",
        "outputId": "df75c473-e620-474d-fb95-58757cf3b43b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['car', 'auto', 'automobile', 'machine', 'motorcar']\n",
            "['car', 'railcar', 'railway_car', 'railroad_car']\n",
            "['car', 'gondola']\n",
            "['car', 'elevator_car']\n",
            "['cable_car', 'car']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Synsets also come with a prose definition and some example sentences\n",
        "for batch in synset:\n",
        "  print(batch.definition())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfyi1iSOY7_v",
        "outputId": "d6982cd0-d255-4431-897a-4f57705cdb58"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
            "a wheeled vehicle adapted to the rails of railroad\n",
            "the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant\n",
            "where passengers ride up and down\n",
            "a conveyance for passengers or freight on a cable railway\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in synset:\n",
        "  print(batch.examples())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcSIEUFuZUhi",
        "outputId": "ca89ee51-c36a-4da0-b0b9-6d11ccb549cc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['he needs a car to get to work']\n",
            "['three cars had jumped the rails']\n",
            "[]\n",
            "['the car was on the top floor']\n",
            "['they took a cable car to the top of the mountain']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "motocar = wn.synset('car.n.01')\n",
        "types= motocar.hyponyms()\n",
        "types"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVfD63gvlbWO",
        "outputId": "7629dc91-900c-4bf5-fa6a-919173cd782b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('ambulance.n.01'),\n",
              " Synset('beach_wagon.n.01'),\n",
              " Synset('bus.n.04'),\n",
              " Synset('cab.n.03'),\n",
              " Synset('compact.n.03'),\n",
              " Synset('convertible.n.01'),\n",
              " Synset('coupe.n.01'),\n",
              " Synset('cruiser.n.01'),\n",
              " Synset('electric.n.01'),\n",
              " Synset('gas_guzzler.n.01'),\n",
              " Synset('hardtop.n.01'),\n",
              " Synset('hatchback.n.01'),\n",
              " Synset('horseless_carriage.n.01'),\n",
              " Synset('hot_rod.n.01'),\n",
              " Synset('jeep.n.01'),\n",
              " Synset('limousine.n.01'),\n",
              " Synset('loaner.n.02'),\n",
              " Synset('minicar.n.01'),\n",
              " Synset('minivan.n.01'),\n",
              " Synset('model_t.n.01'),\n",
              " Synset('pace_car.n.01'),\n",
              " Synset('racer.n.02'),\n",
              " Synset('roadster.n.01'),\n",
              " Synset('sedan.n.01'),\n",
              " Synset('sport_utility.n.01'),\n",
              " Synset('sports_car.n.01'),\n",
              " Synset('stanley_steamer.n.01'),\n",
              " Synset('stock_car.n.01'),\n",
              " Synset('subcompact.n.01'),\n",
              " Synset('touring_car.n.01'),\n",
              " Synset('used-car.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now, based on the response to the previous question: design a program that assigns a numerical similarity score to two input sentences in terms of how similar they are with respect to where the words are within the conceptual hierarchies in WordNet.\n",
        "Instead of using just unigram-level similarity, try to incorporate n-gram aspects of assigning a higher similarity to texts that contain sequences of words that are all similar to one another, lowering the similarity whenever this breaks.\n",
        "For example a small dog that is hungry is very similar to a petite canine who runs in the first four words but then differs at the end.\n",
        "Again, please provide a code snippet and examples (you can reuse the inputs and the outputs of the previous question as examples in this question).**"
      ],
      "metadata": {
        "id": "8Yz2dZp5SG_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/@adriensieg/text-similarities-da019229c894\n",
        "\n",
        "https://newscatcherapi.com/blog/ultimate-guide-to-text-similarity-with-python"
      ],
      "metadata": {
        "id": "k1ruNJnNY2pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentece 1\n",
        "print('Transform: ' + str(' '.join(change_word(sent_1))))\n",
        "print('Sentence: ' + str(sent_1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1iak5UFCneW",
        "outputId": "67479dd5-bf79-4bdd-c493-098223a28dec"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transform: The roof of my domiciliate need revivify .\n",
            "Sentence: The roof of my house needs repair.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_1 = ['The roof of my domiciliate need revivify.', 'The roof of my house needs repair.']"
      ],
      "metadata": {
        "id": "kTqZn44dCpjs"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentece 2\n",
        "print('Transform: ' + str(' '.join(change_word(sent_2))))\n",
        "print('Sentence: ' + str(sent_2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhrwOBO7CE6Q",
        "outputId": "157e221f-97ba-4013-ed9f-a078db2e2fd1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transform: The passably bird rust vegetable from my garden .\n",
            "Sentence: The pretty birds eat vegetables from my garden.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_2 = ['The passably bird are rust my vegetable from my garden.', 'The pretty birds are eating my vegetables from my garden.']"
      ],
      "metadata": {
        "id": "HLb__ilMCH43"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentece 3\n",
        "print('Transform: ' + str(' '.join(change_word(sent_3))))\n",
        "print('Sentence: ' + str(sent_3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIPyNuHYuQZc",
        "outputId": "0928beba-0e32-4b34-a000-99e6f38e2aad"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transform: This lakeshore bungalow is amaze .\n",
            "Sentence: This lakeside cottage is amazing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_3 = ['This lakeshore bungalow is amaze.', 'This lakeside cottage is amazing.']"
      ],
      "metadata": {
        "id": "D-50bg-my_y1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jaccard similarity:"
      ],
      "metadata": {
        "id": "TcFsK39QzcFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(x,y):\n",
        "  \"\"\" returns the jaccard similarity between two lists \"\"\"\n",
        "  intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
        "  union_cardinality = len(set.union(*[set(x), set(y)]))\n",
        "  return intersection_cardinality/float(union_cardinality)"
      ],
      "metadata": {
        "id": "G6L4CPiczfjY"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jaccard_similarity(sentences_1[0], sentences_1[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaNzUOIgCyAE",
        "outputId": "255c825a-a260-4c8c-f2c1-211c294c1ca2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6666666666666666"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jaccard_similarity(sentences_2[0], sentences_2[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vAmpiAOCRwi",
        "outputId": "97425b7b-45dc-4e3b-b126-2675440cd429"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9545454545454546"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jaccard_similarity(sentences_3[0], sentences_3[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrxWmy4wzgya",
        "outputId": "e947dd12-94b7-4cb3-81ac-26b7124e60d5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6818181818181818"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine Similarity:"
      ],
      "metadata": {
        "id": "7u805k46zrkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBhKn5Rx4XCh",
        "outputId": "b70cd2c4-279a-4323-a9c8-a74a5b7740f1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-10-10 15:11:55.242597: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-md==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.4.0/en_core_web_md-3.4.0-py3-none-any.whl (42.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.8 MB 2.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-md==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.0.7)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.9.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (8.1.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (21.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2022.9.24)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (0.0.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_md')"
      ],
      "metadata": {
        "id": "rApQMnbZ3XUU"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_1 = [nlp(sentence).vector for sentence in sentences_1]\n",
        "embeddings_2 = [nlp(sentence).vector for sentence in sentences_2]\n",
        "embeddings_3 = [nlp(sentence).vector for sentence in sentences_3]"
      ],
      "metadata": {
        "id": "VVU-iaUr3HIj"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt, pow, exp\n",
        " \n",
        "def squared_sum(x):\n",
        "  \"\"\" return 3 rounded square rooted value \"\"\"\n",
        " \n",
        "  return round(sqrt(sum([a*a for a in x])),3)"
      ],
      "metadata": {
        "id": "SwRQipi0_p5a"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cos_similarity(x,y):\n",
        "  \"\"\" return cosine similarity between two lists \"\"\"\n",
        " \n",
        "  numerator = sum(a*b for a,b in zip(x,y))\n",
        "  denominator = squared_sum(x)*squared_sum(y)\n",
        "  return round(numerator/float(denominator),3)"
      ],
      "metadata": {
        "id": "P1ucXJHG4nRp"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cos_similarity(embeddings_1[0], embeddings_1[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szPMeBcYC9WA",
        "outputId": "bddcc8d7-828e-41d0-8d0f-a62739b3c66e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.956"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cos_similarity(embeddings_2[0], embeddings_2[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUJlXPP8Cdq3",
        "outputId": "42019425-d02a-4cb2-f5c9-3d343a3fb322"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.973"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cos_similarity(embeddings_3[0], embeddings_3[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqrIxW_KB_aj",
        "outputId": "433267db-0407-4606-9bfc-3a79c698c3ff"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.959"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "J’ai fait le test avec les 3 phrases de la question 1. J’ai appliqué 2 mesures pour se faire : Jaccard Index et Cosine Similarity. Jaccard est rarement utilisée lorsqu’on travaille avec des données textuelles car elle ne fonctionne pas avec les incorporations de texte (se limite à évaluer la similarité lexicale du texte, donc à quel point les documents sont similaires au niveau des mots). La similarité cosine calcul 2 vecteurs comme le cosinus de l’angl. Il détermine si les 2 vecteurs pointent à peu près dans la même direction. Ainsi on peut voir que le Jaccard index ne donne pas de très bon résultat pour la phrase 1 & 3. Le cosine semble être effectivement une meilleure mesure. Les résultats sont très similaires pour chaque phrase (0.96 à 0.97). Je suis surprise cependant de voir que c’est la phrase qui fait moins de sens qui a le score le plus élevé (phrase 2 – 0.973)."
      ],
      "metadata": {
        "id": "yRgSMfE2iX6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ppVAAiP3hcuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pour voir la similarité entre 2 mots:\n",
        "right = wn.synset('right_whale.n.01')\n",
        "minke = wn.synset('minke_whale.n.01')\n",
        "right"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qpEVF3du75j",
        "outputId": "b216ad03-322a-4bab-c85e-905c15548a85"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Synset('right_whale.n.01')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "right.path_similarity(minke)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViiRz55xvBf1",
        "outputId": "9ab51aa0-dc1b-4a59-d8ec-0cf3124094d5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.25"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = [ 'lakeshore', 'lakeside', 'bungalow', 'cottage', 'amaze', 'amazing']\n",
        "wss = [ wn.synsets(w) for w in words ]\n",
        "for (w, ss) in zip(words, wss):\n",
        "  for s in ss:\n",
        "    print(f'{w} is {s.min_depth()} down from {s.root_hypernyms()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c9S8kjiK5Y6",
        "outputId": "5bb3165e-ecd5-4542-c677-48c11474988c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lakeshore is 5 down from [Synset('entity.n.01')]\n",
            "lakeside is 5 down from [Synset('entity.n.01')]\n",
            "bungalow is 8 down from [Synset('entity.n.01')]\n",
            "cottage is 8 down from [Synset('entity.n.01')]\n",
            "amaze is 2 down from [Synset('affect.v.05')]\n",
            "amaze is 2 down from [Synset('be.v.01')]\n",
            "amazing is 2 down from [Synset('affect.v.05')]\n",
            "amazing is 2 down from [Synset('be.v.01')]\n",
            "amazing is 0 down from [Synset('amazing.s.01')]\n",
            "amazing is 0 down from [Synset('amazing.s.02')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d = dict(zip(words, wss))\n",
        "similarities = dict()\n",
        "for w1 in words:\n",
        "  ss1 = d[w1]\n",
        "  for w2 in words:\n",
        "    if w1 == w2:\n",
        "      continue\n",
        "    ss2 = d[w2]\n",
        "    for (s1, s2) in zip(ss1, ss2): \n",
        "      r1 = s1.root_hypernyms()[0]\n",
        "      r2 = s2.root_hypernyms()[0]\n",
        "      if r1 == r2:\n",
        "        sim = s1.path_similarity(s2) # a value in [0, 1], 1 meaning \"the same\"\n",
        "        key = (w1, w2, r1)\n",
        "        value = max(sim, similarities.get(key, 0)) # highest \n",
        "        similarities[key] = value\n",
        "\n",
        "for (w1, w2, root), value in similarities.items():\n",
        "  print(f'Similarity of {w1} with {w2} is {value} in the hierarchy of {root}')\n",
        "\n",
        "# extreme cases\n",
        "print('Highest similarity', max(similarities, key = similarities.get))\n",
        "print('Lowest similarity', min(similarities, key = similarities.get))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzJVUKngLLo4",
        "outputId": "be2835d6-a3a8-466f-d8f6-f64fb6d808d4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity of lakeshore with lakeside is 1.0 in the hierarchy of Synset('entity.n.01')\n",
            "Similarity of lakeshore with bungalow is 0.1 in the hierarchy of Synset('entity.n.01')\n",
            "Similarity of lakeshore with cottage is 0.1 in the hierarchy of Synset('entity.n.01')\n",
            "Similarity of lakeside with lakeshore is 1.0 in the hierarchy of Synset('entity.n.01')\n",
            "Similarity of lakeside with bungalow is 0.1 in the hierarchy of Synset('entity.n.01')\n",
            "Similarity of lakeside with cottage is 0.1 in the hierarchy of Synset('entity.n.01')\n",
            "Similarity of bungalow with lakeshore is 0.1 in the hierarchy of Synset('entity.n.01')\n",
            "Similarity of bungalow with lakeside is 0.1 in the hierarchy of Synset('entity.n.01')\n",
            "Similarity of bungalow with cottage is 1.0 in the hierarchy of Synset('entity.n.01')\n",
            "Similarity of cottage with lakeshore is 0.1 in the hierarchy of Synset('entity.n.01')\n",
            "Similarity of cottage with lakeside is 0.1 in the hierarchy of Synset('entity.n.01')\n",
            "Similarity of cottage with bungalow is 1.0 in the hierarchy of Synset('entity.n.01')\n",
            "Similarity of amaze with amazing is 1.0 in the hierarchy of Synset('affect.v.05')\n",
            "Similarity of amaze with amazing is 1.0 in the hierarchy of Synset('be.v.01')\n",
            "Similarity of amazing with amaze is 1.0 in the hierarchy of Synset('affect.v.05')\n",
            "Similarity of amazing with amaze is 1.0 in the hierarchy of Synset('be.v.01')\n",
            "Highest similarity ('lakeshore', 'lakeside', Synset('entity.n.01'))\n",
            "Lowest similarity ('lakeshore', 'bungalow', Synset('entity.n.01'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using the Open Multilingual Wordnet at http://compling.hss.ntu.edu.sg/omw/, write a pro- gram that takes as input a sentence along with information about what language this sentence is written in and what language to translate it to, and then, using WordNet to map concepts, write a very rough automated translator.\n",
        "Provide a code snippet, input-output examples, and a discussion on the aspects of language translations that are hard or impossible to capture just using a WordNet as a knowledge base.**"
      ],
      "metadata": {
        "id": "QAWOl57DvCrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence, lang_from, lang_to):\n",
        "\n",
        "  sent=[]\n",
        "\n",
        "  # Tokenize the sentence\n",
        "  sent_1w = word_tokenize(sentence)\n",
        "\n",
        "  # need put definition of each language for each stopword\n",
        "  sp = str()\n",
        "  if lang_from == 'fra': sp = 'french'\n",
        "  stop_words = set(stopwords.words(sp))\n",
        "\n",
        "  for i in range(0, len(sent_1w)):\n",
        "\n",
        "    if sent_1w[i] not in stop_words:\n",
        "      synset = wn.synsets(sent_1w[i].lower(), lang = lang_from)\n",
        "\n",
        "        # check is the word have synset\n",
        "      if len(synset) == 0:\n",
        "        continue\n",
        "\n",
        "        # if the word have synset, take a similar word\n",
        "      else:\n",
        "        for j in range (0, len(synset)):\n",
        "          lm = synset[j].lemma_names(lang = lang_to)\n",
        "          if len(lm) == 0:\n",
        "            continue\n",
        "          else:\n",
        "            sent.append(lm[-1])\n",
        "            break\n",
        "\n",
        "  return sent"
      ],
      "metadata": {
        "id": "oq8pCLovwUWE"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'Le chien joue avec sa balle.'"
      ],
      "metadata": {
        "id": "4aLnqAXlv6-G"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(sentence, 'fra', 'ita')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1_qRqD2xKcs",
        "outputId": "b63dbc08-fe0b-450b-90f6-665e7c2aaf74"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cane', 'temerarietà', 'palla_veloce']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mon programme ne prend pas en considération les stopwords. Puisque ma phrase est en français, j’ai défini les stopwords dans cette langue mais il faudrait ajouter une liste de stopwords pour chaque langue utilisée.\n",
        "\n",
        "Phrase : Le chien joue avec sa balle. \n",
        "Mot transformé en italien : chien = cane, joue = temerarietà, balle = palla_veloce.\n",
        "\n",
        "Les stopwords ne sont pas définie dans WordNet, ainsi il est difficile de faire la translation d’une phrase complète. Le mot chien est bien transformé d’une langue à une autre, ce qui n’est cependant pas le cas avec le mot joue (insouciance) et balle (balle rapide) qui change la signification de la phrase et devient moins compréhensible pour l’humain. Problématique :\n",
        "-\tComment déterminer automatiquement quel synset à utiliser.\n",
        "\n",
        "https://aclanthology.org/P10-4014.pdf"
      ],
      "metadata": {
        "id": "LJHkStxjn25Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "synset = wn.synsets('avec',lang='fra')\n",
        "for batch in synset:\n",
        "  print(batch.lemma_names())\n",
        "synset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycOAInC6kKXT",
        "outputId": "634c82b0-d0a9-422c-9aee-7252ab1019e7"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    }
  ]
}
